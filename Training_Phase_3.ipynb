{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_Phase-3.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvHzvpdMZf3k",
        "outputId": "98afed07-5321-4cb2-fc4f-8113228c81ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.7/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install and import the required packages\n",
        "from distutils.dir_util import copy_tree\n",
        "copy_tree(\"/content/drive/MyDrive/Capstone_Project/Capstone Project/results/lib\", \"/content/drive/MyDrive/Capstone_Project/Capstone Project/Working\")\n",
        "!pip install efficientnet_pytorch \n",
        "\n",
        "import os\n",
        "import csv\n",
        "import albumentations as A\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Capstone_Project/Capstone Project/results/lib\")\n",
        "\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import OneCycleLR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"opencv-python-headless<4.3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TJeTxAJuqG2",
        "outputId": "5c7dfe8c-8af4-45fd-b9af-092e41eafa76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python-headless<4.3\n",
            "  Downloading opencv_python_headless-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (21.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless<4.3) (1.21.5)\n",
            "Installing collected packages: opencv-python-headless\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.5.5.64\n",
            "    Uninstalling opencv-python-headless-4.5.5.64:\n",
            "      Successfully uninstalled opencv-python-headless-4.5.5.64\n",
            "Successfully installed opencv-python-headless-4.2.0.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data import CLASSES\n",
        "from models import get_unet\n",
        "from training import load_segmentation_transforms, cycle_seg_classifier, save_state\n",
        "from models import CLASSIFIER_MODEL_GENERATORS\n",
        "from datasets import OrthonetClassificationDataset"
      ],
      "metadata": {
        "id": "CX0K3n7nafl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# True to train only 1 model architecture\n",
        "STOP_AFTER_EFFICIENTNET = True"
      ],
      "metadata": {
        "id": "71Njd1ePaluL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "CSV_TRAIN_VAL = \"/content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/Capstone_Project/Capstone Project/Working\"\n",
        "DATA_PATH = \"/content/drive/MyDrive/Capstone_Project/Capstone Project/archive/orthonet data new\"\n",
        "UNET_PATH = \"/content/drive/MyDrive/Capstone_Project/Capstone Project/Trained_Model_Phase-2/seg_unet_192_0.9000270.pt\"\n",
        "\n",
        "# Other settings\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "WEIGHT_LOSS = True\n",
        "FOLD = 3\n",
        "BS_TRAIN = 8\n",
        "BS_VAL = 8\n",
        "N_WORKERS = 2\n",
        "N_EPOCHS = 230\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 5e-4\n",
        "\n",
        "USE_CLAHE = True  # Adaptive histogram normalisation after segmentation to accentuate implant features"
      ],
      "metadata": {
        "id": "amG0JnsWaplY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Vv8JDug97eiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_by_model_by_epoch = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "clahe_transform = A.Compose([A.CLAHE(p=1)]) if USE_CLAHE else None\n",
        "\n",
        "unet_model = get_unet(1, 1)\n",
        "unet_model.load_state_dict(torch.load(UNET_PATH)['state_dict'])\n",
        "unet_model = unet_model.to(DEVICE)\n",
        "\n",
        "for model_type, model_generator in CLASSIFIER_MODEL_GENERATORS.items():\n",
        "\n",
        "    # Data\n",
        "    train_transforms, test_transforms = load_segmentation_transforms()\n",
        "    ds_train = OrthonetClassificationDataset('train', CSV_TRAIN_VAL, DATA_PATH, train_transforms)\n",
        "    ds_val = OrthonetClassificationDataset('val', CSV_TRAIN_VAL, DATA_PATH, test_transforms)\n",
        "    dl_train = DataLoader(ds_train, BS_TRAIN, shuffle=True, num_workers=N_WORKERS, pin_memory=True)\n",
        "    dl_val = DataLoader(ds_val, BS_VAL, shuffle=True, num_workers=N_WORKERS, pin_memory=True)\n",
        "\n",
        "    print(f\"TRAIN\")\n",
        "    ds_train.stats()\n",
        "    print(f\"VAL\")\n",
        "    ds_val.stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOyceVHxbEWR",
        "outputId": "51fade23-326d-4b13-ab06-69ceb938a87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 918 train samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "Found 251 val samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "TRAIN\n",
            "396 unique patients\n",
            "Class                                             Number of samples\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               51\n",
            "Knee_SmithAndNephew_GenesisII                     117\n",
            "Hip_Stryker_Exeter                                192\n",
            "Knee_Depuy_Synthes_Sigma                          78\n",
            "Hip_DepuySynthes_Corail_Collar                    102\n",
            "Hip_DepuySynthes_Corail_NilCol                    128\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              22\n",
            "Knee_SmithAndNephew_Legion2                       29\n",
            "Hip_Stryker_AccoladeII                            34\n",
            "Hip_SmithAndNephew_Anthology                      88\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              22\n",
            "Knee_ZimmerBiomet_Oxford                          55\n",
            "\n",
            "\n",
            "VAL\n",
            "98 unique patients\n",
            "Class                                             Number of samples\n",
            "Knee_SmithAndNephew_GenesisII                     40\n",
            "Hip_SmithAndNephew_Anthology                      14\n",
            "Knee_Depuy_Synthes_Sigma                          36\n",
            "Hip_Stryker_Exeter                                26\n",
            "Hip_DepuySynthes_Corail_NilCol                    19\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               21\n",
            "Hip_Stryker_AccoladeII                            14\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              27\n",
            "Hip_DepuySynthes_Corail_Collar                    37\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              5\n",
            "Knee_ZimmerBiomet_Oxford                          12\n",
            "\n",
            "\n",
            "Found 918 train samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "Found 251 val samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "TRAIN\n",
            "396 unique patients\n",
            "Class                                             Number of samples\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               51\n",
            "Knee_SmithAndNephew_GenesisII                     117\n",
            "Hip_Stryker_Exeter                                192\n",
            "Knee_Depuy_Synthes_Sigma                          78\n",
            "Hip_DepuySynthes_Corail_Collar                    102\n",
            "Hip_DepuySynthes_Corail_NilCol                    128\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              22\n",
            "Knee_SmithAndNephew_Legion2                       29\n",
            "Hip_Stryker_AccoladeII                            34\n",
            "Hip_SmithAndNephew_Anthology                      88\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              22\n",
            "Knee_ZimmerBiomet_Oxford                          55\n",
            "\n",
            "\n",
            "VAL\n",
            "98 unique patients\n",
            "Class                                             Number of samples\n",
            "Knee_SmithAndNephew_GenesisII                     40\n",
            "Hip_SmithAndNephew_Anthology                      14\n",
            "Knee_Depuy_Synthes_Sigma                          36\n",
            "Hip_Stryker_Exeter                                26\n",
            "Hip_DepuySynthes_Corail_NilCol                    19\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               21\n",
            "Hip_Stryker_AccoladeII                            14\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              27\n",
            "Hip_DepuySynthes_Corail_Collar                    37\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              5\n",
            "Knee_ZimmerBiomet_Oxford                          12\n",
            "\n",
            "\n",
            "Found 918 train samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "Found 251 val samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "TRAIN\n",
            "396 unique patients\n",
            "Class                                             Number of samples\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               51\n",
            "Knee_SmithAndNephew_GenesisII                     117\n",
            "Hip_Stryker_Exeter                                192\n",
            "Knee_Depuy_Synthes_Sigma                          78\n",
            "Hip_DepuySynthes_Corail_Collar                    102\n",
            "Hip_DepuySynthes_Corail_NilCol                    128\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              22\n",
            "Knee_SmithAndNephew_Legion2                       29\n",
            "Hip_Stryker_AccoladeII                            34\n",
            "Hip_SmithAndNephew_Anthology                      88\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              22\n",
            "Knee_ZimmerBiomet_Oxford                          55\n",
            "\n",
            "\n",
            "VAL\n",
            "98 unique patients\n",
            "Class                                             Number of samples\n",
            "Knee_SmithAndNephew_GenesisII                     40\n",
            "Hip_SmithAndNephew_Anthology                      14\n",
            "Knee_Depuy_Synthes_Sigma                          36\n",
            "Hip_Stryker_Exeter                                26\n",
            "Hip_DepuySynthes_Corail_NilCol                    19\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               21\n",
            "Hip_Stryker_AccoladeII                            14\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              27\n",
            "Hip_DepuySynthes_Corail_Collar                    37\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              5\n",
            "Knee_ZimmerBiomet_Oxford                          12\n",
            "\n",
            "\n",
            "Found 918 train samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "Found 251 val samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "TRAIN\n",
            "396 unique patients\n",
            "Class                                             Number of samples\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               51\n",
            "Knee_SmithAndNephew_GenesisII                     117\n",
            "Hip_Stryker_Exeter                                192\n",
            "Knee_Depuy_Synthes_Sigma                          78\n",
            "Hip_DepuySynthes_Corail_Collar                    102\n",
            "Hip_DepuySynthes_Corail_NilCol                    128\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              22\n",
            "Knee_SmithAndNephew_Legion2                       29\n",
            "Hip_Stryker_AccoladeII                            34\n",
            "Hip_SmithAndNephew_Anthology                      88\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              22\n",
            "Knee_ZimmerBiomet_Oxford                          55\n",
            "\n",
            "\n",
            "VAL\n",
            "98 unique patients\n",
            "Class                                             Number of samples\n",
            "Knee_SmithAndNephew_GenesisII                     40\n",
            "Hip_SmithAndNephew_Anthology                      14\n",
            "Knee_Depuy_Synthes_Sigma                          36\n",
            "Hip_Stryker_Exeter                                26\n",
            "Hip_DepuySynthes_Corail_NilCol                    19\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               21\n",
            "Hip_Stryker_AccoladeII                            14\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              27\n",
            "Hip_DepuySynthes_Corail_Collar                    37\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              5\n",
            "Knee_ZimmerBiomet_Oxford                          12\n",
            "\n",
            "\n",
            "Found 918 train samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "Found 251 val samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "TRAIN\n",
            "396 unique patients\n",
            "Class                                             Number of samples\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               51\n",
            "Knee_SmithAndNephew_GenesisII                     117\n",
            "Hip_Stryker_Exeter                                192\n",
            "Knee_Depuy_Synthes_Sigma                          78\n",
            "Hip_DepuySynthes_Corail_Collar                    102\n",
            "Hip_DepuySynthes_Corail_NilCol                    128\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              22\n",
            "Knee_SmithAndNephew_Legion2                       29\n",
            "Hip_Stryker_AccoladeII                            34\n",
            "Hip_SmithAndNephew_Anthology                      88\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              22\n",
            "Knee_ZimmerBiomet_Oxford                          55\n",
            "\n",
            "\n",
            "VAL\n",
            "98 unique patients\n",
            "Class                                             Number of samples\n",
            "Knee_SmithAndNephew_GenesisII                     40\n",
            "Hip_SmithAndNephew_Anthology                      14\n",
            "Knee_Depuy_Synthes_Sigma                          36\n",
            "Hip_Stryker_Exeter                                26\n",
            "Hip_DepuySynthes_Corail_NilCol                    19\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               21\n",
            "Hip_Stryker_AccoladeII                            14\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              27\n",
            "Hip_DepuySynthes_Corail_Collar                    37\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              5\n",
            "Knee_ZimmerBiomet_Oxford                          12\n",
            "\n",
            "\n",
            "Found 918 train samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "Found 251 val samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "TRAIN\n",
            "396 unique patients\n",
            "Class                                             Number of samples\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               51\n",
            "Knee_SmithAndNephew_GenesisII                     117\n",
            "Hip_Stryker_Exeter                                192\n",
            "Knee_Depuy_Synthes_Sigma                          78\n",
            "Hip_DepuySynthes_Corail_Collar                    102\n",
            "Hip_DepuySynthes_Corail_NilCol                    128\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              22\n",
            "Knee_SmithAndNephew_Legion2                       29\n",
            "Hip_Stryker_AccoladeII                            34\n",
            "Hip_SmithAndNephew_Anthology                      88\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              22\n",
            "Knee_ZimmerBiomet_Oxford                          55\n",
            "\n",
            "\n",
            "VAL\n",
            "98 unique patients\n",
            "Class                                             Number of samples\n",
            "Knee_SmithAndNephew_GenesisII                     40\n",
            "Hip_SmithAndNephew_Anthology                      14\n",
            "Knee_Depuy_Synthes_Sigma                          36\n",
            "Hip_Stryker_Exeter                                26\n",
            "Hip_DepuySynthes_Corail_NilCol                    19\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               21\n",
            "Hip_Stryker_AccoladeII                            14\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              27\n",
            "Hip_DepuySynthes_Corail_Collar                    37\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              5\n",
            "Knee_ZimmerBiomet_Oxford                          12\n",
            "\n",
            "\n",
            "Found 918 train samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "Found 251 val samples from /content/drive/MyDrive/Capstone_Project/Capstone Project/archive/train.csv\n",
            "\n",
            "TRAIN\n",
            "396 unique patients\n",
            "Class                                             Number of samples\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               51\n",
            "Knee_SmithAndNephew_GenesisII                     117\n",
            "Hip_Stryker_Exeter                                192\n",
            "Knee_Depuy_Synthes_Sigma                          78\n",
            "Hip_DepuySynthes_Corail_Collar                    102\n",
            "Hip_DepuySynthes_Corail_NilCol                    128\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              22\n",
            "Knee_SmithAndNephew_Legion2                       29\n",
            "Hip_Stryker_AccoladeII                            34\n",
            "Hip_SmithAndNephew_Anthology                      88\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              22\n",
            "Knee_ZimmerBiomet_Oxford                          55\n",
            "\n",
            "\n",
            "VAL\n",
            "98 unique patients\n",
            "Class                                             Number of samples\n",
            "Knee_SmithAndNephew_GenesisII                     40\n",
            "Hip_SmithAndNephew_Anthology                      14\n",
            "Knee_Depuy_Synthes_Sigma                          36\n",
            "Hip_Stryker_Exeter                                26\n",
            "Hip_DepuySynthes_Corail_NilCol                    19\n",
            "Hip_SmithAndNephew_Polarstem_NilCol               21\n",
            "Hip_Stryker_AccoladeII                            14\n",
            "Hip_JRIOrtho_FurlongEvolution_Collar              27\n",
            "Hip_DepuySynthes_Corail_Collar                    37\n",
            "Hip_JRIOrtho_FurlongEvolution_NilCol              5\n",
            "Knee_ZimmerBiomet_Oxford                          12\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "j5KBsPmcg0J3",
        "outputId": "754c873d-90fc-4251-bf03-c369ed1bcc6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.21.5)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (3.10.0.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
            "Installing collected packages: opencv-python-headless, qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.1.0 opencv-python-headless-4.5.5.64 qudida-0.0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "albumentations",
                  "cv2"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Model\n",
        "    model = model_generator(n_in=2, n_out=len(CLASSES)).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW((p for p in model.parameters() if p.requires_grad), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE*10, steps_per_epoch=len(dl_train), epochs=N_EPOCHS)\n",
        "    train_criterion = nn.CrossEntropyLoss(weight=ds_train.get_class_weights().to(DEVICE) if WEIGHT_LOSS else None)\n",
        "    test_criterion = nn.CrossEntropyLoss(weight=ds_train.get_class_weights().to(DEVICE) if WEIGHT_LOSS else None)\n",
        "\n",
        "    # Train\n",
        "    best_loss, best_path, last_save_path = 1e10, None, None\n",
        "\n",
        "    print(f\"Training {model_type}\")\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "        train_loss, train_acc = cycle_seg_classifier('train', model, unet_model, dl_train, DEVICE, train_criterion, optimizer, scheduler, stack=True, clahe_transform=clahe_transform)\n",
        "        val_loss, val_acc = cycle_seg_classifier('test', model, unet_model, dl_val, DEVICE, test_criterion, optimizer, stack=True, clahe_transform=clahe_transform)\n",
        "\n",
        "        print(f\"Epoch {epoch:03d}\\t\\tTRAIN loss: {train_loss:.4f}\\tTRAIN acc: {train_acc:.4f}\\t\\tVAL loss: {val_loss:.4f}{'*' if val_loss < best_loss else ''}\\tVAL acc: {val_acc:.4f}\")\n",
        "\n",
        "        state = {'epoch': epoch + 1,\n",
        "                 'state_dict': model.state_dict(),\n",
        "                 'optimizer': optimizer.state_dict(),\n",
        "                 'scheduler': scheduler}\n",
        "        save_path = os.path.join(MODEL_DIR, f\"segclass_s_{model_type}_{epoch}_{val_loss:.07f}.pt\")\n",
        "        best_loss, last_save_path = save_state(state, save_path, val_loss, best_loss, last_save_path)\n",
        "\n",
        "        results_by_model_by_epoch[model_type]['train_loss'].append(train_loss)\n",
        "        results_by_model_by_epoch[model_type]['train_acc'].append(train_acc)\n",
        "        results_by_model_by_epoch[model_type]['val_loss'].append(val_loss)\n",
        "        results_by_model_by_epoch[model_type]['val_acc'].append(val_acc)\n",
        "\n",
        "    with open(os.path.join(MODEL_DIR, f\"segclass_s_{model_type}_{best_loss}.txt\"), 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(results_by_model_by_epoch[model_type].keys())\n",
        "        writer.writerows(zip(*results_by_model_by_epoch[model_type].values()))\n",
        "        \n",
        "    if STOP_AFTER_EFFICIENTNET:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0Hn2g2CNgH-Z",
        "outputId": "e70d5821-f1b7-4e3c-a6c5-5cedd72d37b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training wideresnet50\n",
            "Epoch 001\t\tTRAIN loss: 2.4044\tTRAIN acc: 0.2069\t\tVAL loss: 2.6341*\tVAL acc: 0.1289\n",
            "Epoch 002\t\tTRAIN loss: 2.2080\tTRAIN acc: 0.2649\t\tVAL loss: 2.1014*\tVAL acc: 0.2721\n",
            "Epoch 003\t\tTRAIN loss: 2.0067\tTRAIN acc: 0.3254\t\tVAL loss: 2.2662\tVAL acc: 0.2292\n",
            "Epoch 004\t\tTRAIN loss: 1.8313\tTRAIN acc: 0.3475\t\tVAL loss: 2.1855\tVAL acc: 0.2734\n",
            "Epoch 005\t\tTRAIN loss: 1.7631\tTRAIN acc: 0.3851\t\tVAL loss: 1.7894*\tVAL acc: 0.3659\n",
            "Epoch 006\t\tTRAIN loss: 1.8043\tTRAIN acc: 0.3504\t\tVAL loss: 1.8702\tVAL acc: 0.3620\n",
            "Epoch 007\t\tTRAIN loss: 1.7228\tTRAIN acc: 0.3953\t\tVAL loss: 2.0102\tVAL acc: 0.2930\n",
            "Epoch 008\t\tTRAIN loss: 1.5796\tTRAIN acc: 0.4344\t\tVAL loss: 1.9870\tVAL acc: 0.3034\n",
            "Epoch 009\t\tTRAIN loss: 1.5203\tTRAIN acc: 0.4620\t\tVAL loss: 1.7885*\tVAL acc: 0.4466\n",
            "Epoch 010\t\tTRAIN loss: 1.5073\tTRAIN acc: 0.4844\t\tVAL loss: 1.8730\tVAL acc: 0.3529\n",
            "Epoch 011\t\tTRAIN loss: 1.4745\tTRAIN acc: 0.4993\t\tVAL loss: 1.5252*\tVAL acc: 0.5156\n",
            "Epoch 012\t\tTRAIN loss: 1.3691\tTRAIN acc: 0.5272\t\tVAL loss: 1.8417\tVAL acc: 0.3438\n",
            "Epoch 013\t\tTRAIN loss: 1.4004\tTRAIN acc: 0.5080\t\tVAL loss: 1.8636\tVAL acc: 0.3633\n",
            "Epoch 014\t\tTRAIN loss: 1.4119\tTRAIN acc: 0.5087\t\tVAL loss: 1.9728\tVAL acc: 0.5169\n",
            "Epoch 015\t\tTRAIN loss: 1.4371\tTRAIN acc: 0.5275\t\tVAL loss: 1.7150\tVAL acc: 0.4167\n",
            "Epoch 016\t\tTRAIN loss: 1.3764\tTRAIN acc: 0.5272\t\tVAL loss: 1.7390\tVAL acc: 0.3229\n",
            "Epoch 017\t\tTRAIN loss: 1.2923\tTRAIN acc: 0.5710\t\tVAL loss: 1.7902\tVAL acc: 0.4701\n",
            "Epoch 018\t\tTRAIN loss: 1.2848\tTRAIN acc: 0.5772\t\tVAL loss: 1.4883*\tVAL acc: 0.5026\n",
            "Epoch 019\t\tTRAIN loss: 1.3380\tTRAIN acc: 0.5471\t\tVAL loss: 1.7736\tVAL acc: 0.3919\n",
            "Epoch 020\t\tTRAIN loss: 1.2314\tTRAIN acc: 0.5993\t\tVAL loss: 1.7954\tVAL acc: 0.3424\n",
            "Epoch 021\t\tTRAIN loss: 1.2136\tTRAIN acc: 0.6286\t\tVAL loss: 2.0423\tVAL acc: 0.2891\n",
            "Epoch 022\t\tTRAIN loss: 1.0980\tTRAIN acc: 0.6598\t\tVAL loss: 1.6192\tVAL acc: 0.5052\n",
            "Epoch 023\t\tTRAIN loss: 1.1915\tTRAIN acc: 0.6239\t\tVAL loss: 1.7924\tVAL acc: 0.5182\n",
            "Epoch 024\t\tTRAIN loss: 1.0318\tTRAIN acc: 0.6536\t\tVAL loss: 1.3414*\tVAL acc: 0.4622\n",
            "Epoch 025\t\tTRAIN loss: 1.1400\tTRAIN acc: 0.6395\t\tVAL loss: 2.1322\tVAL acc: 0.2604\n",
            "Epoch 026\t\tTRAIN loss: 1.1278\tTRAIN acc: 0.6261\t\tVAL loss: 1.1431*\tVAL acc: 0.6211\n",
            "Epoch 027\t\tTRAIN loss: 1.0474\tTRAIN acc: 0.6540\t\tVAL loss: 1.4165\tVAL acc: 0.5026\n",
            "Epoch 028\t\tTRAIN loss: 1.0734\tTRAIN acc: 0.6507\t\tVAL loss: 1.4204\tVAL acc: 0.4792\n",
            "Epoch 029\t\tTRAIN loss: 1.0912\tTRAIN acc: 0.6428\t\tVAL loss: 1.1857\tVAL acc: 0.6445\n",
            "Epoch 030\t\tTRAIN loss: 1.1006\tTRAIN acc: 0.6453\t\tVAL loss: 1.2891\tVAL acc: 0.5143\n",
            "Epoch 031\t\tTRAIN loss: 1.0662\tTRAIN acc: 0.6533\t\tVAL loss: 1.3760\tVAL acc: 0.5469\n",
            "Epoch 032\t\tTRAIN loss: 1.0516\tTRAIN acc: 0.6616\t\tVAL loss: 0.9727*\tVAL acc: 0.7474\n",
            "Epoch 033\t\tTRAIN loss: 1.2001\tTRAIN acc: 0.6304\t\tVAL loss: 1.0350\tVAL acc: 0.6224\n",
            "Epoch 034\t\tTRAIN loss: 0.8979\tTRAIN acc: 0.7101\t\tVAL loss: 1.0524\tVAL acc: 0.6810\n",
            "Epoch 035\t\tTRAIN loss: 1.0499\tTRAIN acc: 0.6475\t\tVAL loss: 1.4545\tVAL acc: 0.5951\n",
            "Epoch 036\t\tTRAIN loss: 1.0095\tTRAIN acc: 0.6804\t\tVAL loss: 1.0038\tVAL acc: 0.6797\n",
            "Epoch 037\t\tTRAIN loss: 0.9695\tTRAIN acc: 0.6880\t\tVAL loss: 1.4614\tVAL acc: 0.5352\n",
            "Epoch 038\t\tTRAIN loss: 0.9723\tTRAIN acc: 0.6953\t\tVAL loss: 0.8249*\tVAL acc: 0.7305\n",
            "Epoch 039\t\tTRAIN loss: 0.9636\tTRAIN acc: 0.6768\t\tVAL loss: 1.0862\tVAL acc: 0.6406\n",
            "Epoch 040\t\tTRAIN loss: 0.9611\tTRAIN acc: 0.6877\t\tVAL loss: 1.3417\tVAL acc: 0.5365\n",
            "Epoch 041\t\tTRAIN loss: 1.0208\tTRAIN acc: 0.6822\t\tVAL loss: 1.1550\tVAL acc: 0.5911\n",
            "Epoch 042\t\tTRAIN loss: 0.8776\tTRAIN acc: 0.7149\t\tVAL loss: 1.4939\tVAL acc: 0.4167\n",
            "Epoch 043\t\tTRAIN loss: 0.9991\tTRAIN acc: 0.6786\t\tVAL loss: 1.0808\tVAL acc: 0.6797\n",
            "Epoch 044\t\tTRAIN loss: 0.8798\tTRAIN acc: 0.7348\t\tVAL loss: 1.4527\tVAL acc: 0.5286\n",
            "Epoch 045\t\tTRAIN loss: 0.9993\tTRAIN acc: 0.6663\t\tVAL loss: 2.2668\tVAL acc: 0.2513\n",
            "Epoch 046\t\tTRAIN loss: 0.8762\tTRAIN acc: 0.7130\t\tVAL loss: 0.9704\tVAL acc: 0.6771\n",
            "Epoch 047\t\tTRAIN loss: 0.9071\tTRAIN acc: 0.7149\t\tVAL loss: 0.7647*\tVAL acc: 0.7695\n",
            "Epoch 048\t\tTRAIN loss: 0.8041\tTRAIN acc: 0.7413\t\tVAL loss: 1.0265\tVAL acc: 0.6810\n",
            "Epoch 049\t\tTRAIN loss: 0.8931\tTRAIN acc: 0.7225\t\tVAL loss: 1.1352\tVAL acc: 0.5716\n",
            "Epoch 050\t\tTRAIN loss: 0.8683\tTRAIN acc: 0.7337\t\tVAL loss: 0.9040\tVAL acc: 0.7331\n",
            "Epoch 051\t\tTRAIN loss: 0.9898\tTRAIN acc: 0.7011\t\tVAL loss: 1.4226\tVAL acc: 0.4857\n",
            "Epoch 052\t\tTRAIN loss: 0.8120\tTRAIN acc: 0.7301\t\tVAL loss: 1.8548\tVAL acc: 0.4193\n",
            "Epoch 053\t\tTRAIN loss: 0.7611\tTRAIN acc: 0.7572\t\tVAL loss: 0.9611\tVAL acc: 0.6719\n",
            "Epoch 054\t\tTRAIN loss: 0.8661\tTRAIN acc: 0.7315\t\tVAL loss: 1.5230\tVAL acc: 0.4167\n",
            "Epoch 055\t\tTRAIN loss: 0.9605\tTRAIN acc: 0.6967\t\tVAL loss: 1.5406\tVAL acc: 0.5990\n",
            "Epoch 056\t\tTRAIN loss: 0.7782\tTRAIN acc: 0.7529\t\tVAL loss: 0.9401\tVAL acc: 0.6914\n",
            "Epoch 057\t\tTRAIN loss: 0.7362\tTRAIN acc: 0.7721\t\tVAL loss: 0.8854\tVAL acc: 0.7266\n",
            "Epoch 058\t\tTRAIN loss: 0.8416\tTRAIN acc: 0.7257\t\tVAL loss: 1.0703\tVAL acc: 0.7305\n",
            "Epoch 059\t\tTRAIN loss: 0.7705\tTRAIN acc: 0.7536\t\tVAL loss: 1.0713\tVAL acc: 0.6875\n",
            "Epoch 060\t\tTRAIN loss: 0.8261\tTRAIN acc: 0.7301\t\tVAL loss: 1.0428\tVAL acc: 0.7188\n",
            "Epoch 061\t\tTRAIN loss: 0.7002\tTRAIN acc: 0.7851\t\tVAL loss: 0.6894*\tVAL acc: 0.8086\n",
            "Epoch 062\t\tTRAIN loss: 0.8333\tTRAIN acc: 0.7373\t\tVAL loss: 1.1516\tVAL acc: 0.6367\n",
            "Epoch 063\t\tTRAIN loss: 0.6985\tTRAIN acc: 0.7667\t\tVAL loss: 0.9311\tVAL acc: 0.7383\n",
            "Epoch 064\t\tTRAIN loss: 0.7353\tTRAIN acc: 0.7656\t\tVAL loss: 1.1710\tVAL acc: 0.5560\n",
            "Epoch 065\t\tTRAIN loss: 0.7541\tTRAIN acc: 0.7572\t\tVAL loss: 0.9256\tVAL acc: 0.6589\n",
            "Epoch 066\t\tTRAIN loss: 0.7354\tTRAIN acc: 0.7681\t\tVAL loss: 0.7878\tVAL acc: 0.7305\n",
            "Epoch 067\t\tTRAIN loss: 0.7523\tTRAIN acc: 0.7580\t\tVAL loss: 0.9925\tVAL acc: 0.6641\n",
            "Epoch 068\t\tTRAIN loss: 0.7120\tTRAIN acc: 0.7736\t\tVAL loss: 1.0092\tVAL acc: 0.6602\n",
            "Epoch 069\t\tTRAIN loss: 0.6669\tTRAIN acc: 0.7880\t\tVAL loss: 0.8692\tVAL acc: 0.7578\n",
            "Epoch 070\t\tTRAIN loss: 0.7977\tTRAIN acc: 0.7504\t\tVAL loss: 1.1420\tVAL acc: 0.6510\n",
            "Epoch 071\t\tTRAIN loss: 0.7105\tTRAIN acc: 0.7746\t\tVAL loss: 1.1250\tVAL acc: 0.6029\n",
            "Epoch 072\t\tTRAIN loss: 0.6666\tTRAIN acc: 0.7880\t\tVAL loss: 1.0085\tVAL acc: 0.6198\n",
            "Epoch 073\t\tTRAIN loss: 0.6570\tTRAIN acc: 0.7790\t\tVAL loss: 1.0256\tVAL acc: 0.7148\n",
            "Epoch 074\t\tTRAIN loss: 0.7114\tTRAIN acc: 0.7750\t\tVAL loss: 0.8243\tVAL acc: 0.7865\n",
            "Epoch 075\t\tTRAIN loss: 0.5862\tTRAIN acc: 0.8072\t\tVAL loss: 0.7568\tVAL acc: 0.7266\n",
            "Epoch 076\t\tTRAIN loss: 0.6987\tTRAIN acc: 0.7558\t\tVAL loss: 0.8313\tVAL acc: 0.7669\n",
            "Epoch 077\t\tTRAIN loss: 0.5845\tTRAIN acc: 0.8036\t\tVAL loss: 0.8790\tVAL acc: 0.7578\n",
            "Epoch 078\t\tTRAIN loss: 0.6548\tTRAIN acc: 0.7920\t\tVAL loss: 0.8329\tVAL acc: 0.7305\n",
            "Epoch 079\t\tTRAIN loss: 0.6656\tTRAIN acc: 0.7859\t\tVAL loss: 1.0395\tVAL acc: 0.7122\n",
            "Epoch 080\t\tTRAIN loss: 0.5732\tTRAIN acc: 0.8094\t\tVAL loss: 0.7690\tVAL acc: 0.7878\n",
            "Epoch 081\t\tTRAIN loss: 0.5798\tTRAIN acc: 0.7946\t\tVAL loss: 0.6515*\tVAL acc: 0.8516\n",
            "Epoch 082\t\tTRAIN loss: 0.6230\tTRAIN acc: 0.7953\t\tVAL loss: 0.6188*\tVAL acc: 0.7930\n",
            "Epoch 083\t\tTRAIN loss: 0.5678\tTRAIN acc: 0.8156\t\tVAL loss: 0.7568\tVAL acc: 0.7148\n",
            "Epoch 084\t\tTRAIN loss: 0.5852\tTRAIN acc: 0.8130\t\tVAL loss: 0.7894\tVAL acc: 0.7279\n",
            "Epoch 085\t\tTRAIN loss: 0.6225\tTRAIN acc: 0.8018\t\tVAL loss: 0.7232\tVAL acc: 0.8125\n",
            "Epoch 086\t\tTRAIN loss: 0.5923\tTRAIN acc: 0.8040\t\tVAL loss: 0.7819\tVAL acc: 0.7552\n",
            "Epoch 087\t\tTRAIN loss: 0.5417\tTRAIN acc: 0.8181\t\tVAL loss: 0.7884\tVAL acc: 0.7174\n",
            "Epoch 088\t\tTRAIN loss: 0.6116\tTRAIN acc: 0.8058\t\tVAL loss: 0.6519\tVAL acc: 0.8242\n",
            "Epoch 089\t\tTRAIN loss: 0.5424\tTRAIN acc: 0.8290\t\tVAL loss: 0.4947*\tVAL acc: 0.8529\n",
            "Epoch 090\t\tTRAIN loss: 0.4583\tTRAIN acc: 0.8406\t\tVAL loss: 0.5019\tVAL acc: 0.8333\n",
            "Epoch 091\t\tTRAIN loss: 0.5781\tTRAIN acc: 0.8083\t\tVAL loss: 0.5693\tVAL acc: 0.7891\n",
            "Epoch 092\t\tTRAIN loss: 0.5575\tTRAIN acc: 0.8196\t\tVAL loss: 0.7957\tVAL acc: 0.7201\n",
            "Epoch 093\t\tTRAIN loss: 0.5359\tTRAIN acc: 0.8312\t\tVAL loss: 0.8201\tVAL acc: 0.7669\n",
            "Epoch 094\t\tTRAIN loss: 0.5124\tTRAIN acc: 0.8344\t\tVAL loss: 0.9111\tVAL acc: 0.7318\n",
            "Epoch 095\t\tTRAIN loss: 0.5396\tTRAIN acc: 0.8322\t\tVAL loss: 0.7295\tVAL acc: 0.7656\n",
            "Epoch 096\t\tTRAIN loss: 0.5463\tTRAIN acc: 0.8159\t\tVAL loss: 0.5930\tVAL acc: 0.8594\n",
            "Epoch 097\t\tTRAIN loss: 0.5267\tTRAIN acc: 0.8301\t\tVAL loss: 0.5261\tVAL acc: 0.8490\n",
            "Epoch 098\t\tTRAIN loss: 0.4886\tTRAIN acc: 0.8533\t\tVAL loss: 1.1152\tVAL acc: 0.6992\n",
            "Epoch 099\t\tTRAIN loss: 0.6096\tTRAIN acc: 0.8185\t\tVAL loss: 0.5437\tVAL acc: 0.8867\n",
            "Epoch 100\t\tTRAIN loss: 0.4366\tTRAIN acc: 0.8678\t\tVAL loss: 0.5057\tVAL acc: 0.8555\n",
            "Epoch 101\t\tTRAIN loss: 0.4488\tTRAIN acc: 0.8384\t\tVAL loss: 0.6911\tVAL acc: 0.8047\n",
            "Epoch 102\t\tTRAIN loss: 0.5335\tTRAIN acc: 0.8297\t\tVAL loss: 0.7475\tVAL acc: 0.8125\n",
            "Epoch 103\t\tTRAIN loss: 0.4318\tTRAIN acc: 0.8591\t\tVAL loss: 0.5970\tVAL acc: 0.8099\n",
            "Epoch 104\t\tTRAIN loss: 0.4121\tTRAIN acc: 0.8627\t\tVAL loss: 1.0859\tVAL acc: 0.6849\n",
            "Epoch 105\t\tTRAIN loss: 0.5021\tTRAIN acc: 0.8428\t\tVAL loss: 0.5756\tVAL acc: 0.8255\n",
            "Epoch 106\t\tTRAIN loss: 0.4476\tTRAIN acc: 0.8572\t\tVAL loss: 0.8615\tVAL acc: 0.8073\n",
            "Epoch 107\t\tTRAIN loss: 0.4987\tTRAIN acc: 0.8551\t\tVAL loss: 0.6828\tVAL acc: 0.7812\n",
            "Epoch 108\t\tTRAIN loss: 0.4028\tTRAIN acc: 0.8801\t\tVAL loss: 0.3729*\tVAL acc: 0.8906\n",
            "Epoch 109\t\tTRAIN loss: 0.4765\tTRAIN acc: 0.8572\t\tVAL loss: 0.5477\tVAL acc: 0.8568\n",
            "Epoch 110\t\tTRAIN loss: 0.4579\tTRAIN acc: 0.8591\t\tVAL loss: 0.6153\tVAL acc: 0.8138\n",
            "Epoch 111\t\tTRAIN loss: 0.4757\tTRAIN acc: 0.8529\t\tVAL loss: 0.5211\tVAL acc: 0.8789\n",
            "Epoch 112\t\tTRAIN loss: 0.4003\tTRAIN acc: 0.8779\t\tVAL loss: 0.4229\tVAL acc: 0.8880\n",
            "Epoch 113\t\tTRAIN loss: 0.4267\tTRAIN acc: 0.8685\t\tVAL loss: 0.4948\tVAL acc: 0.8464\n",
            "Epoch 114\t\tTRAIN loss: 0.3704\tTRAIN acc: 0.8822\t\tVAL loss: 0.6258\tVAL acc: 0.8177\n",
            "Epoch 115\t\tTRAIN loss: 0.3942\tTRAIN acc: 0.8732\t\tVAL loss: 0.4933\tVAL acc: 0.8346\n",
            "Epoch 116\t\tTRAIN loss: 0.4132\tTRAIN acc: 0.8551\t\tVAL loss: 0.6209\tVAL acc: 0.8086\n",
            "Epoch 117\t\tTRAIN loss: 0.3651\tTRAIN acc: 0.8902\t\tVAL loss: 0.4903\tVAL acc: 0.8724\n",
            "Epoch 118\t\tTRAIN loss: 0.3555\tTRAIN acc: 0.8967\t\tVAL loss: 0.4427\tVAL acc: 0.8802\n",
            "Epoch 119\t\tTRAIN loss: 0.3773\tTRAIN acc: 0.8772\t\tVAL loss: 0.6589\tVAL acc: 0.7826\n",
            "Epoch 120\t\tTRAIN loss: 0.4011\tTRAIN acc: 0.8685\t\tVAL loss: 0.4787\tVAL acc: 0.8633\n",
            "Epoch 121\t\tTRAIN loss: 0.3880\tTRAIN acc: 0.8678\t\tVAL loss: 0.5445\tVAL acc: 0.8320\n",
            "Epoch 122\t\tTRAIN loss: 0.3994\tTRAIN acc: 0.8812\t\tVAL loss: 0.4676\tVAL acc: 0.8724\n",
            "Epoch 123\t\tTRAIN loss: 0.3769\tTRAIN acc: 0.8880\t\tVAL loss: 0.6916\tVAL acc: 0.7552\n",
            "Epoch 124\t\tTRAIN loss: 0.3498\tTRAIN acc: 0.8793\t\tVAL loss: 0.4664\tVAL acc: 0.9023\n",
            "Epoch 125\t\tTRAIN loss: 0.3748\tTRAIN acc: 0.8855\t\tVAL loss: 0.4365\tVAL acc: 0.8789\n",
            "Epoch 126\t\tTRAIN loss: 0.3730\tTRAIN acc: 0.8844\t\tVAL loss: 0.4204\tVAL acc: 0.9102\n",
            "Epoch 127\t\tTRAIN loss: 0.3271\tTRAIN acc: 0.9007\t\tVAL loss: 0.4162\tVAL acc: 0.9062\n",
            "Epoch 128\t\tTRAIN loss: 0.3882\tTRAIN acc: 0.8797\t\tVAL loss: 0.5177\tVAL acc: 0.8828\n",
            "Epoch 129\t\tTRAIN loss: 0.3488\tTRAIN acc: 0.8761\t\tVAL loss: 0.2870*\tVAL acc: 0.9453\n",
            "Epoch 130\t\tTRAIN loss: 0.3594\tTRAIN acc: 0.8772\t\tVAL loss: 0.3249\tVAL acc: 0.9310\n",
            "Epoch 131\t\tTRAIN loss: 0.3392\tTRAIN acc: 0.8996\t\tVAL loss: 0.4841\tVAL acc: 0.8516\n",
            "Epoch 132\t\tTRAIN loss: 0.3160\tTRAIN acc: 0.9011\t\tVAL loss: 0.3730\tVAL acc: 0.9258\n",
            "Epoch 133\t\tTRAIN loss: 0.3152\tTRAIN acc: 0.9076\t\tVAL loss: 0.4040\tVAL acc: 0.8802\n",
            "Epoch 134\t\tTRAIN loss: 0.3558\tTRAIN acc: 0.8772\t\tVAL loss: 0.4183\tVAL acc: 0.8711\n",
            "Epoch 135\t\tTRAIN loss: 0.3502\tTRAIN acc: 0.8880\t\tVAL loss: 0.2653*\tVAL acc: 0.9375\n",
            "Epoch 136\t\tTRAIN loss: 0.2982\tTRAIN acc: 0.8870\t\tVAL loss: 0.3554\tVAL acc: 0.8789\n",
            "Epoch 137\t\tTRAIN loss: 0.3210\tTRAIN acc: 0.9000\t\tVAL loss: 0.4917\tVAL acc: 0.8490\n",
            "Epoch 138\t\tTRAIN loss: 0.2985\tTRAIN acc: 0.9000\t\tVAL loss: 0.3840\tVAL acc: 0.8984\n",
            "Epoch 139\t\tTRAIN loss: 0.3501\tTRAIN acc: 0.8812\t\tVAL loss: 0.2772\tVAL acc: 0.9414\n",
            "Epoch 140\t\tTRAIN loss: 0.2678\tTRAIN acc: 0.9040\t\tVAL loss: 0.2790\tVAL acc: 0.9375\n",
            "Epoch 141\t\tTRAIN loss: 0.2699\tTRAIN acc: 0.9141\t\tVAL loss: 0.2532*\tVAL acc: 0.9336\n",
            "Epoch 142\t\tTRAIN loss: 0.3249\tTRAIN acc: 0.8935\t\tVAL loss: 0.3320\tVAL acc: 0.9219\n",
            "Epoch 143\t\tTRAIN loss: 0.3187\tTRAIN acc: 0.8996\t\tVAL loss: 0.3987\tVAL acc: 0.8906\n",
            "Epoch 144\t\tTRAIN loss: 0.3247\tTRAIN acc: 0.9072\t\tVAL loss: 0.4008\tVAL acc: 0.9062\n",
            "Epoch 145\t\tTRAIN loss: 0.3059\tTRAIN acc: 0.9011\t\tVAL loss: 0.2844\tVAL acc: 0.9531\n",
            "Epoch 146\t\tTRAIN loss: 0.2788\tTRAIN acc: 0.9098\t\tVAL loss: 0.3595\tVAL acc: 0.8867\n",
            "Epoch 147\t\tTRAIN loss: 0.3048\tTRAIN acc: 0.9040\t\tVAL loss: 0.5074\tVAL acc: 0.8685\n",
            "Epoch 148\t\tTRAIN loss: 0.3354\tTRAIN acc: 0.8946\t\tVAL loss: 0.3283\tVAL acc: 0.9062\n",
            "Epoch 149\t\tTRAIN loss: 0.3405\tTRAIN acc: 0.8935\t\tVAL loss: 0.4860\tVAL acc: 0.8672\n",
            "Epoch 150\t\tTRAIN loss: 0.2968\tTRAIN acc: 0.9029\t\tVAL loss: 0.3611\tVAL acc: 0.8880\n",
            "Epoch 151\t\tTRAIN loss: 0.2700\tTRAIN acc: 0.9109\t\tVAL loss: 0.3424\tVAL acc: 0.9141\n",
            "Epoch 152\t\tTRAIN loss: 0.2566\tTRAIN acc: 0.9174\t\tVAL loss: 0.2802\tVAL acc: 0.9336\n",
            "Epoch 153\t\tTRAIN loss: 0.3134\tTRAIN acc: 0.9043\t\tVAL loss: 0.4300\tVAL acc: 0.8841\n",
            "Epoch 154\t\tTRAIN loss: 0.2635\tTRAIN acc: 0.9159\t\tVAL loss: 0.2983\tVAL acc: 0.9297\n",
            "Epoch 155\t\tTRAIN loss: 0.2623\tTRAIN acc: 0.9159\t\tVAL loss: 0.2719\tVAL acc: 0.9375\n",
            "Epoch 156\t\tTRAIN loss: 0.3015\tTRAIN acc: 0.9051\t\tVAL loss: 0.2873\tVAL acc: 0.9375\n",
            "Epoch 157\t\tTRAIN loss: 0.2657\tTRAIN acc: 0.9214\t\tVAL loss: 0.2550\tVAL acc: 0.9453\n",
            "Epoch 158\t\tTRAIN loss: 0.2367\tTRAIN acc: 0.9290\t\tVAL loss: 0.3628\tVAL acc: 0.9023\n",
            "Epoch 159\t\tTRAIN loss: 0.2094\tTRAIN acc: 0.9304\t\tVAL loss: 0.3022\tVAL acc: 0.9193\n",
            "Epoch 160\t\tTRAIN loss: 0.2291\tTRAIN acc: 0.9293\t\tVAL loss: 0.2492*\tVAL acc: 0.9427\n",
            "Epoch 161\t\tTRAIN loss: 0.2281\tTRAIN acc: 0.9337\t\tVAL loss: 0.2401*\tVAL acc: 0.9349\n",
            "Epoch 162\t\tTRAIN loss: 0.2645\tTRAIN acc: 0.9130\t\tVAL loss: 0.2558\tVAL acc: 0.9492\n",
            "Epoch 163\t\tTRAIN loss: 0.2356\tTRAIN acc: 0.9344\t\tVAL loss: 0.2617\tVAL acc: 0.9258\n",
            "Epoch 164\t\tTRAIN loss: 0.2103\tTRAIN acc: 0.9428\t\tVAL loss: 0.2409\tVAL acc: 0.9492\n",
            "Epoch 165\t\tTRAIN loss: 0.2384\tTRAIN acc: 0.9239\t\tVAL loss: 0.2575\tVAL acc: 0.9453\n",
            "Epoch 166\t\tTRAIN loss: 0.2511\tTRAIN acc: 0.9217\t\tVAL loss: 0.2799\tVAL acc: 0.9154\n",
            "Epoch 167\t\tTRAIN loss: 0.2263\tTRAIN acc: 0.9315\t\tVAL loss: 0.3683\tVAL acc: 0.8880\n",
            "Epoch 168\t\tTRAIN loss: 0.2122\tTRAIN acc: 0.9312\t\tVAL loss: 0.2560\tVAL acc: 0.9297\n",
            "Epoch 169\t\tTRAIN loss: 0.2234\tTRAIN acc: 0.9304\t\tVAL loss: 0.2533\tVAL acc: 0.9453\n",
            "Epoch 170\t\tTRAIN loss: 0.1972\tTRAIN acc: 0.9380\t\tVAL loss: 0.2621\tVAL acc: 0.9141\n",
            "Epoch 171\t\tTRAIN loss: 0.2690\tTRAIN acc: 0.9109\t\tVAL loss: 0.2875\tVAL acc: 0.9258\n",
            "Epoch 172\t\tTRAIN loss: 0.2221\tTRAIN acc: 0.9246\t\tVAL loss: 0.2074*\tVAL acc: 0.9388\n",
            "Epoch 173\t\tTRAIN loss: 0.1884\tTRAIN acc: 0.9370\t\tVAL loss: 0.2438\tVAL acc: 0.9336\n",
            "Epoch 174\t\tTRAIN loss: 0.1779\tTRAIN acc: 0.9457\t\tVAL loss: 0.2019*\tVAL acc: 0.9570\n",
            "Epoch 175\t\tTRAIN loss: 0.2014\tTRAIN acc: 0.9359\t\tVAL loss: 0.2336\tVAL acc: 0.9375\n",
            "Epoch 176\t\tTRAIN loss: 0.1821\tTRAIN acc: 0.9370\t\tVAL loss: 0.2029\tVAL acc: 0.9609\n",
            "Epoch 177\t\tTRAIN loss: 0.2294\tTRAIN acc: 0.9225\t\tVAL loss: 0.2067\tVAL acc: 0.9688\n",
            "Epoch 178\t\tTRAIN loss: 0.2320\tTRAIN acc: 0.9203\t\tVAL loss: 0.1823*\tVAL acc: 0.9570\n",
            "Epoch 179\t\tTRAIN loss: 0.2427\tTRAIN acc: 0.9246\t\tVAL loss: 0.3830\tVAL acc: 0.8594\n",
            "Epoch 180\t\tTRAIN loss: 0.2494\tTRAIN acc: 0.9257\t\tVAL loss: 0.2713\tVAL acc: 0.9427\n",
            "Epoch 181\t\tTRAIN loss: 0.2067\tTRAIN acc: 0.9283\t\tVAL loss: 0.2859\tVAL acc: 0.9297\n",
            "Epoch 182\t\tTRAIN loss: 0.1679\tTRAIN acc: 0.9489\t\tVAL loss: 0.2375\tVAL acc: 0.9453\n",
            "Epoch 183\t\tTRAIN loss: 0.1908\tTRAIN acc: 0.9391\t\tVAL loss: 0.2709\tVAL acc: 0.9258\n",
            "Epoch 184\t\tTRAIN loss: 0.1815\tTRAIN acc: 0.9402\t\tVAL loss: 0.2250\tVAL acc: 0.9492\n",
            "Epoch 185\t\tTRAIN loss: 0.1545\tTRAIN acc: 0.9529\t\tVAL loss: 0.2017\tVAL acc: 0.9609\n",
            "Epoch 186\t\tTRAIN loss: 0.1466\tTRAIN acc: 0.9522\t\tVAL loss: 0.1894\tVAL acc: 0.9531\n",
            "Epoch 187\t\tTRAIN loss: 0.1888\tTRAIN acc: 0.9391\t\tVAL loss: 0.2135\tVAL acc: 0.9427\n",
            "Epoch 188\t\tTRAIN loss: 0.2081\tTRAIN acc: 0.9348\t\tVAL loss: 0.1589*\tVAL acc: 0.9727\n",
            "Epoch 189\t\tTRAIN loss: 0.1825\tTRAIN acc: 0.9435\t\tVAL loss: 0.1526*\tVAL acc: 0.9727\n",
            "Epoch 190\t\tTRAIN loss: 0.1648\tTRAIN acc: 0.9424\t\tVAL loss: 0.1927\tVAL acc: 0.9505\n",
            "Epoch 191\t\tTRAIN loss: 0.1993\tTRAIN acc: 0.9355\t\tVAL loss: 0.1766\tVAL acc: 0.9688\n",
            "Epoch 192\t\tTRAIN loss: 0.1764\tTRAIN acc: 0.9380\t\tVAL loss: 0.2137\tVAL acc: 0.9414\n",
            "Epoch 193\t\tTRAIN loss: 0.1501\tTRAIN acc: 0.9475\t\tVAL loss: 0.1838\tVAL acc: 0.9492\n",
            "Epoch 194\t\tTRAIN loss: 0.1402\tTRAIN acc: 0.9518\t\tVAL loss: 0.1729\tVAL acc: 0.9688\n",
            "Epoch 195\t\tTRAIN loss: 0.1643\tTRAIN acc: 0.9486\t\tVAL loss: 0.1875\tVAL acc: 0.9609\n",
            "Epoch 196\t\tTRAIN loss: 0.1902\tTRAIN acc: 0.9380\t\tVAL loss: 0.1576\tVAL acc: 0.9727\n",
            "Epoch 197\t\tTRAIN loss: 0.1748\tTRAIN acc: 0.9467\t\tVAL loss: 0.1666\tVAL acc: 0.9570\n",
            "Epoch 198\t\tTRAIN loss: 0.1851\tTRAIN acc: 0.9380\t\tVAL loss: 0.1774\tVAL acc: 0.9531\n",
            "Epoch 199\t\tTRAIN loss: 0.1692\tTRAIN acc: 0.9467\t\tVAL loss: 0.1434*\tVAL acc: 0.9766\n",
            "Epoch 200\t\tTRAIN loss: 0.1884\tTRAIN acc: 0.9391\t\tVAL loss: 0.1530\tVAL acc: 0.9766\n",
            "Epoch 201\t\tTRAIN loss: 0.1932\tTRAIN acc: 0.9402\t\tVAL loss: 0.1636\tVAL acc: 0.9570\n",
            "Epoch 202\t\tTRAIN loss: 0.1360\tTRAIN acc: 0.9572\t\tVAL loss: 0.1500\tVAL acc: 0.9727\n",
            "Epoch 203\t\tTRAIN loss: 0.1932\tTRAIN acc: 0.9402\t\tVAL loss: 0.1333*\tVAL acc: 0.9883\n",
            "Epoch 204\t\tTRAIN loss: 0.1330\tTRAIN acc: 0.9533\t\tVAL loss: 0.2078\tVAL acc: 0.9701\n",
            "Epoch 205\t\tTRAIN loss: 0.1690\tTRAIN acc: 0.9424\t\tVAL loss: 0.1797\tVAL acc: 0.9583\n",
            "Epoch 206\t\tTRAIN loss: 0.1525\tTRAIN acc: 0.9533\t\tVAL loss: 0.1616\tVAL acc: 0.9688\n",
            "Epoch 207\t\tTRAIN loss: 0.1475\tTRAIN acc: 0.9533\t\tVAL loss: 0.1586\tVAL acc: 0.9688\n",
            "Epoch 208\t\tTRAIN loss: 0.1458\tTRAIN acc: 0.9522\t\tVAL loss: 0.1863\tVAL acc: 0.9727\n",
            "Epoch 209\t\tTRAIN loss: 0.1700\tTRAIN acc: 0.9413\t\tVAL loss: 0.1822\tVAL acc: 0.9688\n",
            "Epoch 210\t\tTRAIN loss: 0.1673\tTRAIN acc: 0.9446\t\tVAL loss: 0.1603\tVAL acc: 0.9727\n",
            "Epoch 211\t\tTRAIN loss: 0.1562\tTRAIN acc: 0.9533\t\tVAL loss: 0.1760\tVAL acc: 0.9727\n",
            "Epoch 212\t\tTRAIN loss: 0.1733\tTRAIN acc: 0.9402\t\tVAL loss: 0.1546\tVAL acc: 0.9805\n",
            "Epoch 213\t\tTRAIN loss: 0.1206\tTRAIN acc: 0.9627\t\tVAL loss: 0.1662\tVAL acc: 0.9648\n",
            "Epoch 214\t\tTRAIN loss: 0.1250\tTRAIN acc: 0.9565\t\tVAL loss: 0.1527\tVAL acc: 0.9766\n",
            "Epoch 215\t\tTRAIN loss: 0.1576\tTRAIN acc: 0.9486\t\tVAL loss: 0.1778\tVAL acc: 0.9570\n",
            "Epoch 216\t\tTRAIN loss: 0.1468\tTRAIN acc: 0.9576\t\tVAL loss: 0.1783\tVAL acc: 0.9648\n",
            "Epoch 217\t\tTRAIN loss: 0.1659\tTRAIN acc: 0.9442\t\tVAL loss: 0.1536\tVAL acc: 0.9648\n",
            "Epoch 218\t\tTRAIN loss: 0.1595\tTRAIN acc: 0.9467\t\tVAL loss: 0.1710\tVAL acc: 0.9609\n",
            "Epoch 219\t\tTRAIN loss: 0.1390\tTRAIN acc: 0.9587\t\tVAL loss: 0.1482\tVAL acc: 0.9766\n",
            "Epoch 220\t\tTRAIN loss: 0.1425\tTRAIN acc: 0.9587\t\tVAL loss: 0.1604\tVAL acc: 0.9766\n",
            "Epoch 221\t\tTRAIN loss: 0.1640\tTRAIN acc: 0.9453\t\tVAL loss: 0.1737\tVAL acc: 0.9727\n",
            "Epoch 222\t\tTRAIN loss: 0.1612\tTRAIN acc: 0.9457\t\tVAL loss: 0.1572\tVAL acc: 0.9688\n",
            "Epoch 223\t\tTRAIN loss: 0.1871\tTRAIN acc: 0.9344\t\tVAL loss: 0.1565\tVAL acc: 0.9727\n",
            "Epoch 224\t\tTRAIN loss: 0.1518\tTRAIN acc: 0.9489\t\tVAL loss: 0.1501\tVAL acc: 0.9805\n",
            "Epoch 225\t\tTRAIN loss: 0.1555\tTRAIN acc: 0.9489\t\tVAL loss: 0.1471\tVAL acc: 0.9688\n",
            "Epoch 226\t\tTRAIN loss: 0.1567\tTRAIN acc: 0.9511\t\tVAL loss: 0.1503\tVAL acc: 0.9688\n",
            "Epoch 227\t\tTRAIN loss: 0.1757\tTRAIN acc: 0.9359\t\tVAL loss: 0.1421\tVAL acc: 0.9766\n",
            "Epoch 228\t\tTRAIN loss: 0.1420\tTRAIN acc: 0.9663\t\tVAL loss: 0.1670\tVAL acc: 0.9648\n",
            "Epoch 229\t\tTRAIN loss: 0.1747\tTRAIN acc: 0.9446\t\tVAL loss: 0.1590\tVAL acc: 0.9727\n",
            "Epoch 230\t\tTRAIN loss: 0.1607\tTRAIN acc: 0.9478\t\tVAL loss: 0.1632\tVAL acc: 0.9648\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-00e50a4f5d02>\"\u001b[0;36m, line \u001b[0;32m36\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "xptHVoxb4meX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model,input_size=(1,64,64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "fcFOlWnl40nu",
        "outputId": "d8a50942-b794-4bff-ee23-11a589e765b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ac182641843f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'summary' is not defined"
          ]
        }
      ]
    }
  ]
}