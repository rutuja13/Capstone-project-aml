{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a60c471",
   "metadata": {},
   "source": [
    "# Training Phase-1 (Simple CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c5dfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet_pytorch in c:\\programdata\\anaconda3\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (from efficientnet_pytorch) (1.10.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from distutils.dir_util import copy_tree\n",
    "copy_tree(\"C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//lib\", \"..//Capstone Project//Working//\")\n",
    "!pip install efficientnet_pytorch\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append(\"C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//results//results//lib\")\n",
    "\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c7acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import load_classifier_transforms, cycle, save_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c8eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLASSIFIER_MODEL_GENERATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df54e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import CLASSES\n",
    "from datasets import OrthonetClassificationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4a6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train only 1 model architecture\n",
    "STOP_AFTER_EFFICIENTNET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "058f07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "CSV_TRAIN_VAL = \"C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//data//archive//train.csv\"\n",
    "DATA_PATH = \"C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//data//archive//orthonet data//orthonet data new\"\n",
    "MODEL_DIR = \"C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone Project//Working//\"\n",
    "\n",
    "WEIGHT_LOSS = True\n",
    "BS_TRAIN = 32\n",
    "BS_VAL = 32\n",
    "N_WORKERS = 2\n",
    "N_EPOCHS = 300\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ec4754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 918 train samples from C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//data//archive//train.csv\n",
      "\n",
      "Found 251 val samples from C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//data//archive//train.csv\n",
      "\n",
      "TRAIN\n",
      "396 unique patients\n",
      "Class                                             Number of samples\n",
      "Hip_SmithAndNephew_Polarstem_NilCol               51\n",
      "Knee_SmithAndNephew_GenesisII                     117\n",
      "Hip_Stryker_Exeter                                192\n",
      "Knee_Depuy_Synthes_Sigma                          78\n",
      "Hip_DepuySynthes_Corail_Collar                    102\n",
      "Hip_DepuySynthes_Corail_NilCol                    128\n",
      "Hip_JRIOrtho_FurlongEvolution_Collar              22\n",
      "Knee_SmithAndNephew_Legion2                       29\n",
      "Hip_Stryker_AccoladeII                            34\n",
      "Hip_SmithAndNephew_Anthology                      88\n",
      "Hip_JRIOrtho_FurlongEvolution_NilCol              22\n",
      "Knee_ZimmerBiomet_Oxford                          55\n",
      "\n",
      "\n",
      "VAL\n",
      "98 unique patients\n",
      "Class                                             Number of samples\n",
      "Knee_SmithAndNephew_GenesisII                     40\n",
      "Hip_SmithAndNephew_Anthology                      14\n",
      "Knee_Depuy_Synthes_Sigma                          36\n",
      "Hip_Stryker_Exeter                                26\n",
      "Hip_DepuySynthes_Corail_NilCol                    19\n",
      "Hip_SmithAndNephew_Polarstem_NilCol               21\n",
      "Hip_Stryker_AccoladeII                            14\n",
      "Hip_JRIOrtho_FurlongEvolution_Collar              27\n",
      "Hip_DepuySynthes_Corail_Collar                    37\n",
      "Hip_JRIOrtho_FurlongEvolution_NilCol              5\n",
      "Knee_ZimmerBiomet_Oxford                          12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_train = OrthonetClassificationDataset('train', CSV_TRAIN_VAL, DATA_PATH, None)\n",
    "ds_val = OrthonetClassificationDataset('val', CSV_TRAIN_VAL, DATA_PATH, None)\n",
    "\n",
    "print(f\"TRAIN\")\n",
    "ds_train.stats()\n",
    "print(f\"VAL\")\n",
    "ds_val.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6ef28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 918 train samples from C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//data//archive//train.csv\n",
      "\n",
      "Found 251 val samples from C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//data//archive//train.csv\n",
      "\n",
      "Training efficientnet\n",
      "Epoch 001\t\tTRAIN loss: 2.5215\tTRAIN acc: 0.0505\tVAL loss: 2.4876*\tVAL acc: 0.0000\n",
      "Epoch 002\t\tTRAIN loss: 2.4864\tTRAIN acc: 0.1174\tVAL loss: 2.4884\tVAL acc: 0.1474\n",
      "Epoch 003\t\tTRAIN loss: 2.4973\tTRAIN acc: 0.1524\tVAL loss: 2.4873*\tVAL acc: 0.1460\n",
      "Epoch 004\t\tTRAIN loss: 2.4664\tTRAIN acc: 0.1755\tVAL loss: 2.4889\tVAL acc: 0.1023\n",
      "Epoch 005\t\tTRAIN loss: 2.4638\tTRAIN acc: 0.1646\tVAL loss: 2.4924\tVAL acc: 0.1023\n",
      "Epoch 006\t\tTRAIN loss: 2.4432\tTRAIN acc: 0.1894\tVAL loss: 2.4946\tVAL acc: 0.1045\n",
      "Epoch 007\t\tTRAIN loss: 2.4436\tTRAIN acc: 0.2006\tVAL loss: 2.4993\tVAL acc: 0.1045\n",
      "Epoch 008\t\tTRAIN loss: 2.4370\tTRAIN acc: 0.1949\tVAL loss: 2.4953\tVAL acc: 0.1045\n",
      "Epoch 009\t\tTRAIN loss: 2.4635\tTRAIN acc: 0.1691\tVAL loss: 2.4980\tVAL acc: 0.1030\n",
      "Epoch 010\t\tTRAIN loss: 2.4591\tTRAIN acc: 0.1696\tVAL loss: 2.5007\tVAL acc: 0.1030\n",
      "Epoch 011\t\tTRAIN loss: 2.4474\tTRAIN acc: 0.1912\tVAL loss: 2.5158\tVAL acc: 0.1030\n",
      "Epoch 012\t\tTRAIN loss: 2.4499\tTRAIN acc: 0.1932\tVAL loss: 2.5286\tVAL acc: 0.1037\n",
      "Epoch 013\t\tTRAIN loss: 2.4562\tTRAIN acc: 0.1597\tVAL loss: 2.5275\tVAL acc: 0.1037\n",
      "Epoch 014\t\tTRAIN loss: 2.4565\tTRAIN acc: 0.1684\tVAL loss: 2.5780\tVAL acc: 0.1037\n",
      "Epoch 015\t\tTRAIN loss: 2.4602\tTRAIN acc: 0.1825\tVAL loss: 2.5683\tVAL acc: 0.0757\n",
      "Epoch 016\t\tTRAIN loss: 2.4531\tTRAIN acc: 0.1776\tVAL loss: 2.5496\tVAL acc: 0.0757\n",
      "Epoch 017\t\tTRAIN loss: 2.4465\tTRAIN acc: 0.1895\tVAL loss: 2.6793\tVAL acc: 0.1052\n",
      "Epoch 018\t\tTRAIN loss: 2.4591\tTRAIN acc: 0.1970\tVAL loss: 2.5758\tVAL acc: 0.1030\n",
      "Epoch 019\t\tTRAIN loss: 2.4504\tTRAIN acc: 0.2052\tVAL loss: 2.6053\tVAL acc: 0.1162\n",
      "Epoch 020\t\tTRAIN loss: 2.4510\tTRAIN acc: 0.1745\tVAL loss: 2.5511\tVAL acc: 0.1599\n",
      "Epoch 021\t\tTRAIN loss: 2.4788\tTRAIN acc: 0.1609\tVAL loss: 2.6392\tVAL acc: 0.1528\n",
      "Epoch 022\t\tTRAIN loss: 2.4728\tTRAIN acc: 0.1993\tVAL loss: 2.5905\tVAL acc: 0.1272\n",
      "Epoch 023\t\tTRAIN loss: 2.4700\tTRAIN acc: 0.1776\tVAL loss: 2.6139\tVAL acc: 0.1538\n",
      "Epoch 024\t\tTRAIN loss: 2.4625\tTRAIN acc: 0.1593\tVAL loss: 2.8662\tVAL acc: 0.1030\n",
      "Epoch 025\t\tTRAIN loss: 2.4509\tTRAIN acc: 0.1887\tVAL loss: 2.7126\tVAL acc: 0.1972\n",
      "Epoch 026\t\tTRAIN loss: 2.4391\tTRAIN acc: 0.1860\tVAL loss: 2.8953\tVAL acc: 0.0242\n",
      "Epoch 027\t\tTRAIN loss: 2.4670\tTRAIN acc: 0.1825\tVAL loss: 2.5646\tVAL acc: 0.1076\n",
      "Epoch 028\t\tTRAIN loss: 2.4508\tTRAIN acc: 0.2120\tVAL loss: 2.7547\tVAL acc: 0.1076\n",
      "Epoch 029\t\tTRAIN loss: 2.4400\tTRAIN acc: 0.2035\tVAL loss: 4.3530\tVAL acc: 0.1037\n",
      "Epoch 030\t\tTRAIN loss: 2.4348\tTRAIN acc: 0.1829\tVAL loss: 7.3585\tVAL acc: 0.0913\n",
      "Epoch 031\t\tTRAIN loss: 2.4425\tTRAIN acc: 0.2222\tVAL loss: 3.7448\tVAL acc: 0.1403\n",
      "Epoch 032\t\tTRAIN loss: 2.4603\tTRAIN acc: 0.1916\tVAL loss: 3.5367\tVAL acc: 0.0664\n",
      "Epoch 033\t\tTRAIN loss: 2.4425\tTRAIN acc: 0.2277\tVAL loss: 2.8868\tVAL acc: 0.0561\n",
      "Epoch 034\t\tTRAIN loss: 2.4348\tTRAIN acc: 0.2011\tVAL loss: 2.8222\tVAL acc: 0.0998\n",
      "Epoch 035\t\tTRAIN loss: 2.4406\tTRAIN acc: 0.2138\tVAL loss: 2.6066\tVAL acc: 0.0476\n",
      "Epoch 036\t\tTRAIN loss: 2.4276\tTRAIN acc: 0.2110\tVAL loss: 3.0544\tVAL acc: 0.0881\n",
      "Epoch 037\t\tTRAIN loss: 2.4353\tTRAIN acc: 0.2080\tVAL loss: 2.5342\tVAL acc: 0.0718\n",
      "Epoch 038\t\tTRAIN loss: 2.4369\tTRAIN acc: 0.1986\tVAL loss: 2.4741*\tVAL acc: 0.1513\n",
      "Epoch 039\t\tTRAIN loss: 2.4318\tTRAIN acc: 0.2073\tVAL loss: 2.5769\tVAL acc: 0.1418\n",
      "Epoch 040\t\tTRAIN loss: 2.4402\tTRAIN acc: 0.1899\tVAL loss: 2.4809\tVAL acc: 0.1748\n",
      "Epoch 041\t\tTRAIN loss: 2.4349\tTRAIN acc: 0.1996\tVAL loss: 2.4854\tVAL acc: 0.0913\n",
      "Epoch 042\t\tTRAIN loss: 2.4241\tTRAIN acc: 0.2186\tVAL loss: 2.4866\tVAL acc: 0.1030\n",
      "Epoch 043\t\tTRAIN loss: 2.4316\tTRAIN acc: 0.1976\tVAL loss: 2.4852\tVAL acc: 0.1943\n",
      "Epoch 044\t\tTRAIN loss: 2.4229\tTRAIN acc: 0.1971\tVAL loss: 2.4878\tVAL acc: 0.1684\n",
      "Epoch 045\t\tTRAIN loss: 2.4157\tTRAIN acc: 0.2184\tVAL loss: 2.7663\tVAL acc: 0.1115\n",
      "Epoch 046\t\tTRAIN loss: 2.4266\tTRAIN acc: 0.1953\tVAL loss: 2.5165\tVAL acc: 0.1037\n",
      "Epoch 047\t\tTRAIN loss: 2.4266\tTRAIN acc: 0.2151\tVAL loss: 2.6013\tVAL acc: 0.1638\n",
      "Epoch 048\t\tTRAIN loss: 2.3937\tTRAIN acc: 0.2171\tVAL loss: 2.5070\tVAL acc: 0.1499\n",
      "Epoch 049\t\tTRAIN loss: 2.4396\tTRAIN acc: 0.2100\tVAL loss: 2.4806\tVAL acc: 0.1599\n",
      "Epoch 050\t\tTRAIN loss: 2.4093\tTRAIN acc: 0.2250\tVAL loss: 2.4990\tVAL acc: 0.1723\n",
      "Epoch 051\t\tTRAIN loss: 2.4181\tTRAIN acc: 0.2067\tVAL loss: 2.4613*\tVAL acc: 0.1194\n",
      "Epoch 052\t\tTRAIN loss: 2.4132\tTRAIN acc: 0.2213\tVAL loss: 2.4342*\tVAL acc: 0.1630\n",
      "Epoch 053\t\tTRAIN loss: 2.4222\tTRAIN acc: 0.2159\tVAL loss: 2.6287\tVAL acc: 0.1453\n",
      "Epoch 054\t\tTRAIN loss: 2.4220\tTRAIN acc: 0.1952\tVAL loss: 2.5182\tVAL acc: 0.1091\n",
      "Epoch 055\t\tTRAIN loss: 2.4099\tTRAIN acc: 0.2066\tVAL loss: 2.6457\tVAL acc: 0.0664\n",
      "Epoch 056\t\tTRAIN loss: 2.4139\tTRAIN acc: 0.2201\tVAL loss: 2.5464\tVAL acc: 0.1037\n",
      "Epoch 057\t\tTRAIN loss: 2.4170\tTRAIN acc: 0.2174\tVAL loss: 2.5852\tVAL acc: 0.1179\n",
      "Epoch 058\t\tTRAIN loss: 2.4103\tTRAIN acc: 0.2116\tVAL loss: 2.4751\tVAL acc: 0.1364\n",
      "Epoch 059\t\tTRAIN loss: 2.4118\tTRAIN acc: 0.2239\tVAL loss: 2.4450\tVAL acc: 0.1428\n",
      "Epoch 060\t\tTRAIN loss: 2.3943\tTRAIN acc: 0.2304\tVAL loss: 2.4905\tVAL acc: 0.1662\n",
      "Epoch 061\t\tTRAIN loss: 2.4315\tTRAIN acc: 0.2190\tVAL loss: 2.5112\tVAL acc: 0.1662\n",
      "Epoch 062\t\tTRAIN loss: 2.4149\tTRAIN acc: 0.2099\tVAL loss: 2.5182\tVAL acc: 0.1716\n",
      "Epoch 063\t\tTRAIN loss: 2.4103\tTRAIN acc: 0.2175\tVAL loss: 2.4565\tVAL acc: 0.1233\n",
      "Epoch 064\t\tTRAIN loss: 2.4055\tTRAIN acc: 0.2120\tVAL loss: 2.4683\tVAL acc: 0.1318\n",
      "Epoch 065\t\tTRAIN loss: 2.4140\tTRAIN acc: 0.2046\tVAL loss: 2.5121\tVAL acc: 0.1591\n",
      "Epoch 066\t\tTRAIN loss: 2.4178\tTRAIN acc: 0.2228\tVAL loss: 2.4892\tVAL acc: 0.1435\n",
      "Epoch 067\t\tTRAIN loss: 2.4045\tTRAIN acc: 0.2141\tVAL loss: 2.5051\tVAL acc: 0.1630\n",
      "Epoch 068\t\tTRAIN loss: 2.4135\tTRAIN acc: 0.2249\tVAL loss: 2.4829\tVAL acc: 0.1264\n",
      "Epoch 069\t\tTRAIN loss: 2.4222\tTRAIN acc: 0.2141\tVAL loss: 2.4792\tVAL acc: 0.1364\n",
      "Epoch 070\t\tTRAIN loss: 2.3953\tTRAIN acc: 0.2229\tVAL loss: 2.4903\tVAL acc: 0.1606\n",
      "Epoch 071\t\tTRAIN loss: 2.3974\tTRAIN acc: 0.2146\tVAL loss: 2.5011\tVAL acc: 0.1801\n",
      "Epoch 072\t\tTRAIN loss: 2.3934\tTRAIN acc: 0.2181\tVAL loss: 2.7888\tVAL acc: 0.1620\n",
      "Epoch 073\t\tTRAIN loss: 2.4088\tTRAIN acc: 0.2099\tVAL loss: 2.6378\tVAL acc: 0.1709\n",
      "Epoch 074\t\tTRAIN loss: 2.3950\tTRAIN acc: 0.2252\tVAL loss: 2.4471\tVAL acc: 0.1179\n",
      "Epoch 075\t\tTRAIN loss: 2.3972\tTRAIN acc: 0.2263\tVAL loss: 2.5238\tVAL acc: 0.1926\n",
      "Epoch 076\t\tTRAIN loss: 2.4055\tTRAIN acc: 0.2137\tVAL loss: 2.5345\tVAL acc: 0.1755\n",
      "Epoch 077\t\tTRAIN loss: 2.3954\tTRAIN acc: 0.2305\tVAL loss: 2.5424\tVAL acc: 0.1701\n",
      "Epoch 078\t\tTRAIN loss: 2.4021\tTRAIN acc: 0.2152\tVAL loss: 2.5138\tVAL acc: 0.1965\n",
      "Epoch 079\t\tTRAIN loss: 2.4059\tTRAIN acc: 0.2148\tVAL loss: 2.5002\tVAL acc: 0.1794\n",
      "Epoch 080\t\tTRAIN loss: 2.3980\tTRAIN acc: 0.2206\tVAL loss: 2.5357\tVAL acc: 0.1794\n",
      "Epoch 081\t\tTRAIN loss: 2.3914\tTRAIN acc: 0.2254\tVAL loss: 2.5311\tVAL acc: 0.1840\n",
      "Epoch 082\t\tTRAIN loss: 2.4150\tTRAIN acc: 0.2175\tVAL loss: 2.5150\tVAL acc: 0.1084\n",
      "Epoch 083\t\tTRAIN loss: 2.3965\tTRAIN acc: 0.2256\tVAL loss: 2.4608\tVAL acc: 0.1733\n",
      "Epoch 084\t\tTRAIN loss: 2.4031\tTRAIN acc: 0.2293\tVAL loss: 2.4283*\tVAL acc: 0.2128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results_by_model_by_epoch = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for model_type, model_generator in CLASSIFIER_MODEL_GENERATORS.items():\n",
    "\n",
    "    # Data\n",
    "    train_transforms, test_transforms = load_classifier_transforms()\n",
    "    ds_train = OrthonetClassificationDataset('train', CSV_TRAIN_VAL, DATA_PATH, train_transforms)\n",
    "    ds_val = OrthonetClassificationDataset('val', CSV_TRAIN_VAL, DATA_PATH, test_transforms)\n",
    "    dl_train = DataLoader(ds_train, BS_TRAIN, shuffle=True, num_workers=N_WORKERS, pin_memory=True)\n",
    "    dl_val = DataLoader(ds_val, BS_VAL, shuffle=True, num_workers=N_WORKERS, pin_memory=True)\n",
    "\n",
    "    # Model\n",
    "    model = model_generator(n_in=1, n_out=len(CLASSES)).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW((p for p in model.parameters() if p.requires_grad), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE*10, steps_per_epoch=len(dl_train), epochs=N_EPOCHS)\n",
    "    train_criterion = nn.CrossEntropyLoss(weight=ds_train.get_class_weights().to(DEVICE) if WEIGHT_LOSS else None)\n",
    "    test_criterion = nn.CrossEntropyLoss(weight=ds_train.get_class_weights().to(DEVICE) if WEIGHT_LOSS else None)\n",
    "\n",
    "    # Train\n",
    "    best_loss, best_path, last_save_path = 1e10, None, None\n",
    "\n",
    "    print(f\"Training {model_type}\")\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        train_loss, train_acc = cycle('train', model, dl_train, DEVICE, epoch, train_criterion, optimizer, scheduler)\n",
    "        val_loss, val_acc = cycle('test', model, dl_val, DEVICE, epoch, test_criterion, optimizer)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d}\\t\\tTRAIN loss: {train_loss:.4f}\\tTRAIN acc: {train_acc:.4f}\\tVAL loss: {val_loss:.4f}{'*' if val_loss < best_loss else ''}\\tVAL acc: {val_acc:.4f}\")\n",
    "\n",
    "        state = {'epoch': epoch + 1,\n",
    "                 'state_dict': model.state_dict(),\n",
    "                 'optimizer': optimizer.state_dict(),\n",
    "                 'scheduler': scheduler}\n",
    "        save_path = os.path.join(MODEL_DIR, f\"{model_type}_{epoch}_{val_loss:.07f}.pt\")\n",
    "        best_loss, last_save_path = save_state(state, save_path, val_loss, best_loss, last_save_path)\n",
    "\n",
    "        results_by_model_by_epoch[model_type]['train_loss'].append(train_loss)\n",
    "        results_by_model_by_epoch[model_type]['train_acc'].append(train_acc)\n",
    "        results_by_model_by_epoch[model_type]['val_loss'].append(val_loss)\n",
    "        results_by_model_by_epoch[model_type]['val_acc'].append(val_acc)\n",
    "\n",
    "    with open(os.path.join(MODEL_DIR, f\"{model_type}_{best_loss}.txt\"), 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(results_by_model_by_epoch[model_type].keys())\n",
    "        writer.writerows(zip(*results_by_model_by_epoch[model_type].values()))\n",
    "        \n",
    "    if STOP_AFTER_EFFICIENTNET:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67729f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
