{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a60c471",
   "metadata": {},
   "source": [
    "# Training Phase-1 (Simple CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7c5dfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet_pytorch in c:\\programdata\\anaconda3\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (from efficientnet_pytorch) (1.10.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from distutils.dir_util import copy_tree\n",
    "copy_tree(\"C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//lib\", \"..//Capstone Project//Working//\")\n",
    "!pip install efficientnet_pytorch\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append(\"C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone Project//Working//lib//\")\n",
    "\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42c7acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import load_classifier_transforms, cycle, save_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99c8eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLASSIFIER_MODEL_GENERATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5df54e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import CLASSES\n",
    "from datasets import OrthonetClassificationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a4a6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train only 1 model architecture\n",
    "STOP_AFTER_EFFICIENTNET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "058f07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "CSV_TRAIN_VAL = \"C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//data//archive//train.csv\"\n",
    "DATA_PATH = \"C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//data//archive//orthonet data//orthonet data new\"\n",
    "MODEL_DIR = \"C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone Project//Working\"\n",
    "\n",
    "WEIGHT_LOSS = True\n",
    "BS_TRAIN = 32\n",
    "BS_VAL = 32\n",
    "N_WORKERS = 2\n",
    "N_EPOCHS = 300\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94ec4754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 918 train samples from C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//data//archive//train.csv\n",
      "\n",
      "Found 251 val samples from C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//data//archive//train.csv\n",
      "\n",
      "TRAIN\n",
      "396 unique patients\n",
      "Class                                             Number of samples\n",
      "Hip_SmithAndNephew_Polarstem_NilCol               51\n",
      "Knee_SmithAndNephew_GenesisII                     117\n",
      "Hip_Stryker_Exeter                                192\n",
      "Knee_Depuy_Synthes_Sigma                          78\n",
      "Hip_DepuySynthes_Corail_Collar                    102\n",
      "Hip_DepuySynthes_Corail_NilCol                    128\n",
      "Hip_JRIOrtho_FurlongEvolution_Collar              22\n",
      "Knee_SmithAndNephew_Legion2                       29\n",
      "Hip_Stryker_AccoladeII                            34\n",
      "Hip_SmithAndNephew_Anthology                      88\n",
      "Hip_JRIOrtho_FurlongEvolution_NilCol              22\n",
      "Knee_ZimmerBiomet_Oxford                          55\n",
      "\n",
      "\n",
      "VAL\n",
      "98 unique patients\n",
      "Class                                             Number of samples\n",
      "Knee_SmithAndNephew_GenesisII                     40\n",
      "Hip_SmithAndNephew_Anthology                      14\n",
      "Knee_Depuy_Synthes_Sigma                          36\n",
      "Hip_Stryker_Exeter                                26\n",
      "Hip_DepuySynthes_Corail_NilCol                    19\n",
      "Hip_SmithAndNephew_Polarstem_NilCol               21\n",
      "Hip_Stryker_AccoladeII                            14\n",
      "Hip_JRIOrtho_FurlongEvolution_Collar              27\n",
      "Hip_DepuySynthes_Corail_Collar                    37\n",
      "Hip_JRIOrtho_FurlongEvolution_NilCol              5\n",
      "Knee_ZimmerBiomet_Oxford                          12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_train = OrthonetClassificationDataset('train', CSV_TRAIN_VAL, DATA_PATH, None)\n",
    "ds_val = OrthonetClassificationDataset('val', CSV_TRAIN_VAL, DATA_PATH, None)\n",
    "\n",
    "print(f\"TRAIN\")\n",
    "ds_train.stats()\n",
    "print(f\"VAL\")\n",
    "ds_val.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26b6ef28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 918 train samples from C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//data//archive//train.csv\n",
      "\n",
      "Found 251 val samples from C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone-project-aml//data//archive//train.csv\n",
      "\n",
      "Training efficientnet\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone Project//Working//lib\\datasets.py\", line 95, in __getitem__\n    img = skimage.io.imread(filepath)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\io\\_io.py\", line 48, in imread\n    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\io\\manage_plugins.py\", line 207, in call_plugin\n    return func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py\", line 10, in imread\n    return np.asarray(imageio_imread(*args, **kwargs))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\imageio\\core\\functions.py\", line 265, in imread\n    reader = read(uri, format, \"i\", **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\imageio\\core\\functions.py\", line 172, in get_reader\n    request = Request(uri, \"r\" + mode, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\imageio\\core\\request.py\", line 124, in __init__\n    self._parse_uri(uri)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\imageio\\core\\request.py\", line 260, in _parse_uri\n    raise FileNotFoundError(\"No such file: '%s'\" % fn)\nFileNotFoundError: No such file: 'C:\\Users\\aksha\\Desktop\\Term 3\\project\\Project_work\\Capstone-project-aml\\data\\archive\\orthonet data\\orthonet data new\\0341_12_01_R0123_UNIL.png'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b8b9e28a32eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Training {model_type}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC://Users//aksha//Desktop//Term 3//project//Project_work//Capstone Project//Working//lib\\training.py\u001b[0m in \u001b[0;36mcycle\u001b[1;34m(train_val_test, model, dataloader, device, epoch, criterion, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"train_val_test must be 'train', 'val', or 'test', not {train_val_test}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_label_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1201\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1229\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;31m# instantiate since we don't know how to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C://Users//aksha//Desktop//Term 3//project//Project_work//Capstone Project//Working//lib\\datasets.py\", line 95, in __getitem__\n    img = skimage.io.imread(filepath)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\io\\_io.py\", line 48, in imread\n    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\io\\manage_plugins.py\", line 207, in call_plugin\n    return func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py\", line 10, in imread\n    return np.asarray(imageio_imread(*args, **kwargs))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\imageio\\core\\functions.py\", line 265, in imread\n    reader = read(uri, format, \"i\", **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\imageio\\core\\functions.py\", line 172, in get_reader\n    request = Request(uri, \"r\" + mode, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\imageio\\core\\request.py\", line 124, in __init__\n    self._parse_uri(uri)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\imageio\\core\\request.py\", line 260, in _parse_uri\n    raise FileNotFoundError(\"No such file: '%s'\" % fn)\nFileNotFoundError: No such file: 'C:\\Users\\aksha\\Desktop\\Term 3\\project\\Project_work\\Capstone-project-aml\\data\\archive\\orthonet data\\orthonet data new\\0341_12_01_R0123_UNIL.png'\n"
     ]
    }
   ],
   "source": [
    "results_by_model_by_epoch = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for model_type, model_generator in CLASSIFIER_MODEL_GENERATORS.items():\n",
    "\n",
    "    # Data\n",
    "    train_transforms, test_transforms = load_classifier_transforms()\n",
    "    ds_train = OrthonetClassificationDataset('train', CSV_TRAIN_VAL, DATA_PATH, train_transforms)\n",
    "    ds_val = OrthonetClassificationDataset('val', CSV_TRAIN_VAL, DATA_PATH, test_transforms)\n",
    "    dl_train = DataLoader(ds_train, BS_TRAIN, shuffle=True, num_workers=N_WORKERS, pin_memory=True)\n",
    "    dl_val = DataLoader(ds_val, BS_VAL, shuffle=True, num_workers=N_WORKERS, pin_memory=True)\n",
    "\n",
    "    # Model\n",
    "    model = model_generator(n_in=1, n_out=len(CLASSES)).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW((p for p in model.parameters() if p.requires_grad), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE*10, steps_per_epoch=len(dl_train), epochs=N_EPOCHS)\n",
    "    train_criterion = nn.CrossEntropyLoss(weight=ds_train.get_class_weights().to(DEVICE) if WEIGHT_LOSS else None)\n",
    "    test_criterion = nn.CrossEntropyLoss(weight=ds_train.get_class_weights().to(DEVICE) if WEIGHT_LOSS else None)\n",
    "\n",
    "    # Train\n",
    "    best_loss, best_path, last_save_path = 1e10, None, None\n",
    "\n",
    "    print(f\"Training {model_type}\")\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        train_loss, train_acc = cycle('train', model, dl_train, DEVICE, epoch, train_criterion, optimizer, scheduler)\n",
    "        val_loss, val_acc = cycle('test', model, dl_val, DEVICE, epoch, test_criterion, optimizer)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d}\\t\\tTRAIN loss: {train_loss:.4f}\\tTRAIN acc: {train_acc:.4f}\\tVAL loss: {val_loss:.4f}{'*' if val_loss < best_loss else ''}\\tVAL acc: {val_acc:.4f}\")\n",
    "\n",
    "        state = {'epoch': epoch + 1,\n",
    "                 'state_dict': model.state_dict(),\n",
    "                 'optimizer': optimizer.state_dict(),\n",
    "                 'scheduler': scheduler}\n",
    "        save_path = os.path.join(MODEL_DIR, f\"{model_type}_{epoch}_{val_loss:.07f}.pt\")\n",
    "        best_loss, last_save_path = save_state(state, save_path, val_loss, best_loss, last_save_path)\n",
    "\n",
    "        results_by_model_by_epoch[model_type]['train_loss'].append(train_loss)\n",
    "        results_by_model_by_epoch[model_type]['train_acc'].append(train_acc)\n",
    "        results_by_model_by_epoch[model_type]['val_loss'].append(val_loss)\n",
    "        results_by_model_by_epoch[model_type]['val_acc'].append(val_acc)\n",
    "\n",
    "    with open(os.path.join(MODEL_DIR, f\"{model_type}_{best_loss}.txt\"), 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(results_by_model_by_epoch[model_type].keys())\n",
    "        writer.writerows(zip(*results_by_model_by_epoch[model_type].values()))\n",
    "        \n",
    "    if STOP_AFTER_EFFICIENTNET:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67729f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
